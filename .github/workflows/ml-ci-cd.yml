name: ML CI/CD Pipeline

permissions:
  contents: write

on:
  push:
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.10'
  CACHE_VERSION: v1
  
jobs:
  # Job 1: Code Quality & Linting
  code-quality:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Run PyLint
      id: pylint
      run: |
        echo "Running PyLint with custom configuration..."
        
        # Extract fail-under threshold from .pylintrc
        THRESHOLD=$(grep "fail-under" .pylintrc | sed 's/.*=\s*//' | tr -d ' ')
        echo "PyLint threshold from .pylintrc: $THRESHOLD"
        
        # Run PyLint and extract score
        pylint_output=$(pylint src/ --rcfile=.pylintrc --output-format=text 2>&1 || true)
        pylint_score=$(echo "$pylint_output" | grep "Your code has been rated" | sed 's/.*rated at \([0-9.]*\).*/\1/' || echo "0.0")
        
        echo "pylint_score=$pylint_score" >> $GITHUB_OUTPUT
        echo "threshold=$THRESHOLD" >> $GITHUB_OUTPUT
        echo "PyLint Score: $pylint_score (threshold: $THRESHOLD)"
        
        # Use dynamic threshold comparison
        if python3 -c "import sys; sys.exit(0 if float('$pylint_score') >= float('$THRESHOLD') else 1)"; then
          echo "âœ… PyLint score ($pylint_score) meets threshold ($THRESHOLD)"
        else
          echo "âŒ PyLint score ($pylint_score) is below threshold ($THRESHOLD)"
          echo "Full PyLint output:"
          echo "$pylint_output"
          exit 1
        fi
    
    - name: Run Flake8
      run: |
        echo "Running Flake8 with custom configuration..."
        flake8 src/ --config=.flake8
        echo "âœ… Flake8 passed"
    
    - name: Run Bandit Security Linter
      run: |
        echo "Running Bandit security analysis..."
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ --severity-level medium
        echo "âœ… Bandit security check passed"
    
    - name: Upload linting reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: linting-reports
        path: |
          bandit-report.json
    
    outputs:
      pylint_score: ${{ steps.pylint.outputs.pylint_score }}

  # Job 2: Pre-Pipeline Infrastructure Tests
  infrastructure-tests:
    name: Infrastructure Tests (Pre-Pipeline)
    runs-on: ubuntu-latest
    needs: code-quality
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Setup environment variables
      run: |
        # Create .secrets file for testing
        echo "AWS_ACCESS_KEY_ID=dummy" > .secrets
        echo "AWS_SECRET_ACCESS_KEY=dummy" >> .secrets
        echo "AWS_DEFAULT_REGION=us-east-1" >> .secrets
        echo "HF_TOKEN=${{ secrets.HF_TOKEN }}" >> .secrets
    
    - name: Run Infrastructure Tests with Coverage
      id: infrastructure_testing
      run: |
        echo "Running infrastructure tests before pipeline..."
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        
        coverage run --source=src -m pytest tests/test_infrastructure.py -v \
          --junitxml=infrastructure-test-results.xml \
          --tb=short
        
        coverage report --show-missing | tee infrastructure-coverage.txt
        coverage xml -o infrastructure-coverage.xml
        coverage json -o infrastructure-coverage.json
        
        # Extract coverage percentage more reliably using JSON output
        coverage_percent=$(python -c "import json; data=json.load(open('infrastructure-coverage.json')); print(f'{data[\"totals\"][\"percent_covered\"]:.0f}')" || echo "0")
        echo "infrastructure_coverage=$coverage_percent" >> $GITHUB_OUTPUT
        echo "Infrastructure Test Coverage: $coverage_percent%"
    
    - name: Upload infrastructure test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: infrastructure-test-results
        path: |
          infrastructure-test-results.xml
          infrastructure-coverage.xml
          infrastructure-coverage.txt
          infrastructure-coverage.json
    
    outputs:
      infrastructure_coverage: ${{ steps.infrastructure_testing.outputs.infrastructure_coverage }}

  # Job 3: DVC Pipeline Execution
  dvc-pipeline:
    name: DVC ML Pipeline
    runs-on: ubuntu-latest
    needs: infrastructure-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
        pip install dvc[s3]
    
    - name: Initialize and run DVC pipeline
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "Initializing DVC..."
        dvc init --no-scm --force
        
        echo "Configuring DVC remote..."
        dvc remote add -d storage s3://remla25-team8/dvcstore
        
        echo "Running DVC pipeline..."
        dvc repro --force
        
        echo "Pipeline completed. Checking outputs..."
        ls -la models/ || echo "No models directory"
        ls -la metrics/ || echo "No metrics directory"
        ls -la data/splits/ || echo "No splits directory"
    
    - name: Upload pipeline artifacts
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-artifacts
        path: |
          models/
          metrics/
          data/splits/
          data/raw/
          dvc.lock

  # Job 4: Post-Pipeline Model Tests
  model-tests:
    name: Model Quality Tests (Post-Pipeline)
    runs-on: ubuntu-latest
    needs: dvc-pipeline
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v4
      with:
        name: pipeline-artifacts
        path: .
    
    - name: Run Model Tests with Coverage
      id: model_testing
      run: |
        echo "Running post-pipeline model tests..."
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        
        # Run tests without coverage since we only want infrastructure coverage
        pytest tests/test_model.py -v \
          --junitxml=model-test-results.xml \
          --tb=short
        
        echo "Model tests completed (no coverage measured)"
    
    - name: Upload model test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: model-test-results
        path: |
          model-test-results.xml
    
    outputs:
      # Removed model_coverage output since we're not measuring it

  # Job 5: Mutamorphic Tests
  mutamorphic-tests:
    name: Mutamorphic Tests (Post-Pipeline)
    runs-on: ubuntu-latest
    needs: dvc-pipeline
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v4
      with:
        name: pipeline-artifacts
        path: .
    
    - name: Run Mutamorphic Tests with Coverage
      id: mutamorphic_testing
      run: |
        echo "Running mutamorphic tests..."
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        
        # Run tests without coverage since we only want infrastructure coverage
        pytest tests/test_mutamorphic.py -v \
          --junitxml=mutamorphic-test-results.xml \
          --tb=short
        
        echo "Mutamorphic tests completed (no coverage measured)"
    
    - name: Upload mutamorphic test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: mutamorphic-test-results
        path: |
          mutamorphic-test-results.xml
    
    outputs:
      # Removed mutamorphic_coverage output since we're not measuring it

  # Job 6: Non-Functional Tests
  non-functional-tests:
    name: Non-Functional Tests (Post-Pipeline)
    runs-on: ubuntu-latest
    needs: dvc-pipeline
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v4
      with:
        name: pipeline-artifacts
        path: .
    
    - name: Run Non-Functional Tests with Coverage
      id: nonfunctional_testing
      run: |
        echo "Running non-functional tests..."
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        
        # Run tests without coverage since we only want infrastructure coverage
        pytest tests/test_non_functional.py -v \
          --junitxml=nonfunctional-test-results.xml \
          --tb=short
        
        echo "Non-functional tests completed (no coverage measured)"
    
    - name: Upload non-functional test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: nonfunctional-test-results
        path: |
          nonfunctional-test-results.xml
    
    outputs:
      # Removed nonfunctional_coverage output since we're not measuring it

  # Job 7: Model Upload to Hugging Face (Only on Tags)
  model-upload:
    name: Upload Model to Hugging Face
    runs-on: ubuntu-latest
    needs: [model-tests, mutamorphic-tests, non-functional-tests]
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Download pipeline artifacts
      uses: actions/download-artifact@v4
      with:
        name: pipeline-artifacts
        path: .
    
    - name: Extract tag version
      id: get_version
      run: |
        VERSION=${GITHUB_REF#refs/tags/}
        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "Tag version: $VERSION"
    
    - name: Package model using package_model.py
      run: |
        echo "Packaging model artifacts for release..."
        python src/package_model.py
        
        # Rename with version tag
        mv model_release.zip model_${{ steps.get_version.outputs.version }}.zip
        
        # Set model zip path
        MODEL_ZIP="model_${{ steps.get_version.outputs.version }}.zip"
        echo "MODEL_ZIP=$MODEL_ZIP" >> $GITHUB_ENV
        echo "Created model package: $MODEL_ZIP"
    
    - name: Upload model to Hugging Face
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
      run: |
        echo "Uploading model to Hugging Face..."
        VERSION="${{ steps.get_version.outputs.version }}"
        python src/model_upload.py "$MODEL_ZIP" "$VERSION"
        echo "MODEL_VERSION=$VERSION" >> $GITHUB_ENV
    
    - name: Upload model package artifact
      uses: actions/upload-artifact@v4
      with:
        name: model-package-${{ steps.get_version.outputs.version }}
        path: |
          model_${{ steps.get_version.outputs.version }}.zip
    
    - name: Push model artifacts to DVC remote
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
      run: |
        echo "Configuring DVC remote..."
        dvc remote add -d storage s3://remla25-team8/dvcstore
        
        echo "Pushing trained model and data to DVC remote..."
        dvc push
        
        echo "âœ… Model artifacts pushed to DVC remote storage"
    
    outputs:
      model_version: ${{ env.MODEL_VERSION }}
      model_zip: ${{ env.MODEL_ZIP }}

  # Job 8: Create GitHub Release (Only on Tags)
  github-release:
    name: Create GitHub Release
    runs-on: ubuntu-latest
    needs: model-upload
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Extract tag version
      id: get_version
      run: |
        VERSION=${GITHUB_REF#refs/tags/}
        echo "version=$VERSION" >> $GITHUB_OUTPUT
        echo "Tag version: $VERSION"
    
    - name: Download model package
      uses: actions/download-artifact@v4
      with:
        name: model-package-${{ steps.get_version.outputs.version }}
        path: .
    
    - name: Create GitHub Release
      uses: softprops/action-gh-release@v1
      with:
        tag_name: ${{ steps.get_version.outputs.version }}
        name: Release ${{ steps.get_version.outputs.version }}
        body: |
          ## ðŸš€ Model Release ${{ steps.get_version.outputs.version }}
          
          ### ðŸ“¦ What's included:
          - Trained sentiment classifier model
          - Model metrics and metadata
          - Ready-to-use model package
          
          ### ðŸ¤— Hugging Face:
          This model has been uploaded to Hugging Face with version `${{ needs.model-upload.outputs.model_version }}`
          
          ### ðŸ“Š Quality Metrics:
          - All tests passed âœ…
          - Model quality validated âœ…
          - Mutamorphic tests passed âœ…
          - Non-functional requirements met âœ…
        files: |
          model_${{ steps.get_version.outputs.version }}.zip
        draft: false
        prerelease: false
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Job 9: Test Model Download (Only on Tags)
  test-download:
    name: Test Model Download
    runs-on: ubuntu-latest
    needs: model-upload
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Test model download
      env:
        HF_TOKEN: ${{ secrets.HF_TOKEN }}
        MODEL_VERSION: ${{ needs.model-upload.outputs.model_version }}
      run: |
        echo "Testing model download from Hugging Face..."
        export PYTHONPATH="$PYTHONPATH:$(pwd)/src"
        
        # Wait a bit for the model to be available
        sleep 30
        
        python tests/test_download.py "$MODEL_VERSION"
        echo "âœ… Model download test passed"

  # Job 10: Calculate ML Test Score & Coverage Summary
  ml-test-score:
    name: ML Test Score & Coverage Summary
    runs-on: ubuntu-latest
    needs: [infrastructure-tests, model-tests, mutamorphic-tests, non-functional-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Restore pip cache
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
    
    - name: Install dependencies (if not cached)
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -e .
    
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
    
    - name: Calculate ML Test Score
      id: ml_test_score
      run: |
        echo "Calculating ML Test Score..."
        python scripts/calculate_ml_test_score.py --output-json ml-test-score.json
        
        ml_test_score=$(python -c "import json; data=json.load(open('ml-test-score.json')); print(f'{data[\"final_score\"]}/7')" || echo "0/7")
        echo "ml_test_score=$ml_test_score" >> $GITHUB_OUTPUT
        echo "ML Test Score: $ml_test_score"
        
        python scripts/calculate_ml_test_score.py --verbose
    
    - name: Combine coverage reports
      id: coverage_summary
      env:
        INFRA_COV: ${{ needs.infrastructure-tests.outputs.infrastructure_coverage }}
      run: |
        echo "Using infrastructure coverage as the primary coverage metric..."
        
        # Use infrastructure coverage directly since it's the only one we want
        coverage_percent="$INFRA_COV"
        echo "combined_coverage=$coverage_percent" >> $GITHUB_OUTPUT
        
        echo "Coverage Summary:"
        echo "- Infrastructure Coverage: $INFRA_COV%"
        echo "- Total Coverage: $coverage_percent%"
        echo ""
        echo "Note: Only infrastructure tests measure coverage."
    
    - name: Upload ML test score artifact
      uses: actions/upload-artifact@v4
      with:
        name: ml-test-score
        path: ml-test-score.json
    
    outputs:
      ml_test_score: ${{ steps.ml_test_score.outputs.ml_test_score }}
      combined_coverage: ${{ steps.coverage_summary.outputs.combined_coverage }}

  # Job 11: Update README with Metrics
  update-readme:
    name: Update README with Metrics
    runs-on: ubuntu-latest
    needs: [code-quality, ml-test-score]
    if: github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download ML test score artifact
      uses: actions/download-artifact@v4
      with:
        name: ml-test-score
        path: .
    
    - name: Update README badges and metrics
      env:
        PYLINT_SCORE: ${{ needs.code-quality.outputs.pylint_score }}
        COVERAGE_PERCENT: ${{ needs.ml-test-score.outputs.combined_coverage }}
        ML_TEST_SCORE: ${{ needs.ml-test-score.outputs.ml_test_score }}
      run: |
        echo "Updating README with latest metrics..."
        
        # Determine badge colors based on scores
        pylint_color="red"
        if (( $(echo "$PYLINT_SCORE >= 6.0" | bc -l) )); then
          pylint_color="green"
        elif (( $(echo "$PYLINT_SCORE >= 6.0" | bc -l) )); then
          pylint_color="yellow"
        fi
        
        coverage_color="red"
        if (( $(echo "$COVERAGE_PERCENT >= 80" | bc -l) )); then
          coverage_color="green"
        elif (( $(echo "$COVERAGE_PERCENT >= 60" | bc -l) )); then
          coverage_color="yellow"
        fi
        
        # Download ML test score JSON file
        python scripts/update_readme_badges.py \
          --pylint-score "$PYLINT_SCORE" \
          --pylint-color "$pylint_color" \
          --coverage-percent "$COVERAGE_PERCENT" \
          --coverage-color "$coverage_color" \
          --ml-test-json ml-test-score.json
    
    - name: Commit README updates
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        if git diff --quiet README.md; then
          echo "No changes to README.md"
        else
          echo "Committing updated README.md with latest metrics"
          git add README.md
          git commit -m "ðŸ¤– Auto-update metrics badges [skip ci]"
          git push
        fi

  # Job 12: Generate Final Report
  generate-reports:
    name: Generate Final Report
    runs-on: ubuntu-latest
    needs: [code-quality, ml-test-score, model-upload, github-release]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v4
    
    - name: Generate comprehensive report
      env:
        PYLINT_SCORE: ${{ needs.code-quality.outputs.pylint_score }}
        COVERAGE_PERCENT: ${{ needs.ml-test-score.outputs.combined_coverage }}
        ML_TEST_SCORE: ${{ needs.ml-test-score.outputs.ml_test_score }}
        MODEL_VERSION: ${{ needs.model-upload.outputs.model_version }}
        IS_RELEASE: ${{ startsWith(github.ref, 'refs/tags/v') }}
      run: |
        echo "# ðŸ“Š ML CI/CD Pipeline Report" > pipeline-report.md
        echo "" >> pipeline-report.md
        echo "## ðŸ† Metrics Summary" >> pipeline-report.md
        echo "- **PyLint Score**: $PYLINT_SCORE/10" >> pipeline-report.md
        echo "- **Test Coverage**: $COVERAGE_PERCENT%" >> pipeline-report.md
        echo "- **ML Test Score**: $ML_TEST_SCORE" >> pipeline-report.md
        if [ "$IS_RELEASE" = "true" ]; then
          echo "- **Model Version**: $MODEL_VERSION (Released)" >> pipeline-report.md
        else
          echo "- **Model Version**: Not released (push to branch)" >> pipeline-report.md
        fi
        echo "" >> pipeline-report.md
        echo "## ðŸ§ª Test Execution Order" >> pipeline-report.md
        echo "1. âœ… Infrastructure Tests (Pre-Pipeline)" >> pipeline-report.md
        echo "2. âœ… DVC ML Pipeline" >> pipeline-report.md
        echo "3. âœ… Model Quality Tests (Post-Pipeline)" >> pipeline-report.md
        echo "4. âœ… Mutamorphic Tests (Post-Pipeline)" >> pipeline-report.md
        echo "5. âœ… Non-Functional Tests (Post-Pipeline)" >> pipeline-report.md
        if [ "$IS_RELEASE" = "true" ]; then
          echo "6. âœ… Model Upload to Hugging Face" >> pipeline-report.md
          echo "7. âœ… GitHub Release Created" >> pipeline-report.md
          echo "8. âœ… Model Download Test" >> pipeline-report.md
        else
          echo "6. â­ï¸ Model Upload (Skipped - Not a release)" >> pipeline-report.md
          echo "7. â­ï¸ GitHub Release (Skipped - Not a release)" >> pipeline-report.md
        fi
        echo "" >> pipeline-report.md
        echo "## ðŸ“ˆ Quality Gates" >> pipeline-report.md
        echo "- PyLint: $([ $(echo "$PYLINT_SCORE >= 6.0" | bc -l) = 1 ] && echo "âœ… PASSED" || echo "âŒ FAILED")" >> pipeline-report.md
        echo "- Coverage: $([ $(echo "$COVERAGE_PERCENT >= 60" | bc -l) = 1 ] && echo "âœ… PASSED" || echo "âŒ FAILED")" >> pipeline-report.md
        echo "- ML Tests: $([ "${ML_TEST_SCORE%/*}" -ge 1 ] && echo "âœ… PASSED" || echo "âŒ FAILED")" >> pipeline-report.md
        echo "- Build: $([ "${{ job.status }}" = "success" ] && echo "âœ… PASSED" || echo "âŒ FAILED")" >> pipeline-report.md
        
        cat pipeline-report.md
    
    - name: Upload comprehensive report
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-report
        path: pipeline-report.md
